import re
import pandas as pd
import streamlit as st

# ================== CONFIG ==================
st.set_page_config(page_title="Ù…Ù†ØµØ© Ø£Ø·Ù„Ø³ Ø§Ù„Ø³Ù†Ø© â€“ MVP", layout="wide")
st.write("VERSION: ATLAS-STRICT v0.4")

# ================== Arabic helpers ==================
AR_DIACRITICS = re.compile(r"[\u0617-\u061A\u064B-\u0652\u0670\u06D6-\u06ED]")

def normalize_ar(text: str) -> str:
    if not text:
        return ""
    text = text.strip()
    text = AR_DIACRITICS.sub("", text)
    text = text.replace("Ø£", "Ø§").replace("Ø¥", "Ø§").replace("Ø¢", "Ø§")
    text = text.replace("Ù‰", "ÙŠ").replace("Ø©", "Ù‡")
    text = re.sub(r"[^\w\s\u0600-\u06FF]", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text

def tokenize_ar(text: str):
    return normalize_ar(text).split() if text else []

def similarity_by_reference_words(reference: str, candidate: str) -> float:
    ref_tokens = tokenize_ar(reference)
    cand_tokens = tokenize_ar(candidate)
    if not ref_tokens:
        return 0.0
    ref_set = set(ref_tokens)
    cand_set = set(cand_tokens)
    shared = len(ref_set.intersection(cand_set))
    return (shared / len(ref_set)) * 100.0

def contains_all_words(reference: str, candidate: str) -> bool:
    """
    Ø¨Ø­Ø« Ø£Ø·Ù„Ø³ÙŠ ØµØ§Ø±Ù…:
    True Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù…ØªÙ† ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ØªÙ† Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠ
    """
    ref_tokens = tokenize_ar(reference)
    cand_tokens = tokenize_ar(candidate)
    if not ref_tokens:
        return False
    return all(tok in cand_tokens for tok in ref_tokens)

# ================== Data loading ==================
@st.cache_data
def load_hadith_data():
    try:
        df = pd.read_csv("hadith_data.csv")
        needed = {"hadith_key", "source", "ref", "isnad", "matn"}
        missing = needed - set(df.columns)
        if missing:
            raise ValueError(f"Missing columns: {missing}")
        return df
    except Exception:
        # fallback sample
        sample = [
            {
                "hadith_key": "B00001",
                "source": "Ø¹ÙŠÙ†Ø©",
                "ref": "1",
                "isnad": "ÙÙ„Ø§Ù† Ø¹Ù† ÙÙ„Ø§Ù†",
                "matn": "Ø§Ù†Ù…Ø§ Ø§Ù„Ø§Ø¹Ù…Ø§Ù„ Ø¨Ø§Ù„Ù†ÙŠØ§Øª ÙˆØ§Ù†Ù…Ø§ Ù„ÙƒÙ„ Ø§Ù…Ø±Ø¦ Ù…Ø§ Ù†ÙˆÙ‰"
            },
            {
                "hadith_key": "B00002",
                "source": "Ø¹ÙŠÙ†Ø©",
                "ref": "2",
                "isnad": "ÙÙ„Ø§Ù† Ø¹Ù† ÙÙ„Ø§Ù†",
                "matn": "Ø§Ù„Ø¯ÙŠÙ† Ø§Ù„Ù†ØµÙŠØ­Ù‡ Ù‚Ù„Ù†Ø§ Ù„Ù…Ù† Ù‚Ø§Ù„ Ù„Ù„Ù‡ ÙˆÙ„ÙƒØªØ§Ø¨Ù‡"
            },
        ]
        return pd.DataFrame(sample)

df_hadith = load_hadith_data()

# ================== UI ==================
st.title("ğŸ“š Ù…Ù†ØµØ© Ø£Ø·Ù„Ø³ Ø§Ù„Ø³Ù†Ø© â€“ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø£Ø·Ù„Ø³ÙŠ")

tab1, tab2 = st.tabs(["ğŸ” Ø§Ù„Ø¨Ø­Ø« ÙˆØ¨Ù†Ø§Ø¡ ÙˆØ­Ø¯Ø© Ø§Ù„Ø­Ø¯ÙŠØ«", "â„¹ï¸ Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ù…Ù†Ù‡Ø¬ÙŠØ©"])

# ================== TAB 1 ==================
with tab1:
    st.subheader("ğŸ” Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø£Ø·Ù„Ø³ÙŠ Ø¹Ù† Ø§Ù„Ø­Ø¯ÙŠØ«")

    query = st.text_area(
        "Ù†Øµ Ø§Ù„Ø¨Ø­Ø« (ÙŠÙ…ÙƒÙ† Ø¥Ø¯Ø®Ø§Ù„ Ø£ÙƒØ«Ø± Ù…Ù† Ø­Ø¯ÙŠØ« â€“ ÙƒÙ„ Ø­Ø¯ÙŠØ« ÙÙŠ Ø³Ø·Ø± Ù…Ø³ØªÙ‚Ù„)",
        height=120,
        placeholder="Ù…Ø«Ø§Ù„:\nØ§Ù„Ø¯ÙŠÙ† Ø§Ù„Ù†ØµÙŠØ­Ø©\nØ¥Ù†Ù…Ø§ Ø§Ù„Ø£Ø¹Ù…Ø§Ù„ Ø¨Ø§Ù„Ù†ÙŠØ§Øª"
    )

    col1, col2, col3 = st.columns(3)
    with col1:
        min_sim = st.slider("Ùª Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù„Ù„ØªØ´Ø§Ø¨Ù‡ (Ù„Ù„Ø¨Ø­Ø« ØºÙŠØ± Ø§Ù„ØµØ§Ø±Ù…)", 10, 100, 50, 5)
    with col2:
        top_k = st.slider("Ø¹Ø¯Ø¯ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ (ÙÙŠ Ø§Ù„ÙˆØ¶Ø¹ ØºÙŠØ± Ø§Ù„Ø£Ø·Ù„Ø³ÙŠ)", 5, 200, 30, 5)
    with col3:
        atlas_mode = st.checkbox("ğŸ§­ ÙˆØ¶Ø¹ Ø§Ù„Ø£Ø·Ù„Ø³ (Ø¹Ø±Ø¶ ÙƒÙ„ Ø§Ù„Ø·Ø±Ù‚)", value=True)

    strict_all_words = st.checkbox(
        "ğŸ”’ Ø¨Ø­Ø« Ø£Ø·Ù„Ø³ÙŠ ØµØ§Ø±Ù… (ÙŠØ´ØªØ±Ø· ÙˆØ¬ÙˆØ¯ Ø¬Ù…ÙŠØ¹ ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø­Ø¯ÙŠØ«)",
        value=True
    )

    search_mode = st.selectbox(
        "Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø¨Ø­Ø« (Ø¹Ù†Ø¯ ØªØ¹Ø·ÙŠÙ„ Ø§Ù„ØµØ±Ø§Ù…Ø©)",
        ["Ø§Ù„Ø§Ø«Ù†ÙŠÙ† Ù…Ø¹Ù‹Ø§ (Ø£ÙØ¶Ù„)", "Ø§Ø­ØªÙˆØ§Ø¡ Ø§Ù„Ù†Øµ", "ØªØ´Ø§Ø¨Ù‡ Ø¨Ø§Ù„ÙƒÙ„Ù…Ø§Øª"],
        index=0
    )

    if st.button("Ø§Ø¨Ø­Ø«", type="primary"):
        if not query.strip():
            st.warning("Ø§ÙƒØªØ¨ Ù†ØµÙ‹Ø§ Ù„Ù„Ø¨Ø­Ø« Ø£ÙˆÙ„Ù‹Ø§.")
        else:
            from rapidfuzz import fuzz

            queries = [q.strip() for q in query.split("\n") if q.strip()]
            df = df_hadith.copy()
            df["matn_norm"] = df["matn"].astype(str).apply(normalize_ar)

            all_results = []

            for q in queries:
                q_norm = normalize_ar(q)
                temp = df.copy()

                temp["contains_all"] = temp["matn"].astype(str).apply(
                    lambda x: contains_all_words(q_norm, x)
                )

                temp["contains"] = temp["matn_norm"].apply(
                    lambda x: q_norm in x
                )

                temp["similarity"] = temp["matn"].astype(str).apply(
                    lambda x: similarity_by_reference_words(q_norm, x)
                )

                if strict_all_words:
                    temp = temp[temp["contains_all"]]
                else:
                    if search_mode == "Ø§Ø­ØªÙˆØ§Ø¡ Ø§Ù„Ù†Øµ":
                        temp = temp[temp["contains"]]
                    elif search_mode == "ØªØ´Ø§Ø¨Ù‡ Ø¨Ø§Ù„ÙƒÙ„Ù…Ø§Øª":
                        temp = temp[temp["similarity"] >= float(min_sim)]
                    else:
                        temp = temp[
                            (temp["contains"]) |
                            (temp["similarity"] >= float(min_sim))
                        ]

                all_results.append(temp)

            results = pd.concat(all_results).drop_duplicates()

            if not atlas_mode:
                results = results.sort_values(
                    ["similarity"],
                    ascending=[False]
                ).head(int(top_k))
            else:
                results = results.sort_values(["hadith_key", "ref"])

            if results.empty:
                st.error("Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØªØ§Ø¦Ø¬ Ù…Ø·Ø§Ø¨Ù‚Ø©.")
            else:
                st.success(
                    f"ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(results)} Ø±ÙˆØ§ÙŠØ© Ø¶Ù…Ù† "
                    f"{results['hadith_key'].nunique()} ÙˆØ­Ø¯Ø© Ø­Ø¯ÙŠØ«ÙŠØ© Ø£Ø·Ù„Ø³ÙŠØ©."
                )

                for hadith_key, grp in results.groupby("hadith_key"):
                    with st.expander(
                        f"ğŸ§­ ÙˆØ­Ø¯Ø© Ø­Ø¯ÙŠØ« Ø£Ø·Ù„Ø³ÙŠØ©: {hadith_key} | Ø¹Ø¯Ø¯ Ø§Ù„Ø·Ø±Ù‚: {len(grp)}",
                        expanded=True
                    ):
                        st.markdown("### ğŸ“Œ Ø§Ù„Ù…ØªÙ† Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠ (Raw)")
                        st.write(grp.iloc[0]["matn"])

                        st.markdown("### ğŸ§¾ Ø§Ù„Ø·Ø±Ù‚ ÙˆØ§Ù„Ø±ÙˆØ§ÙŠØ§Øª")
                        for _, r in grp.iterrows():
                            st.markdown(
                                f"- **Ø§Ù„Ù…ØµØ¯Ø±:** {r['source']} | **Ø§Ù„Ù…Ø±Ø¬Ø¹:** {r['ref']}\n"
                                f"  - **Ø§Ù„Ø³Ù†Ø¯ (Raw):** {r['isnad']}\n"
                                f"  - **Ø§Ù„Ù…ØªÙ† (Raw):** {r['matn']}"
                            )

# ================== TAB 2 ==================
with tab2:
    st.info(
        "ğŸ”¹ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ù†ØµØ© Ø£Ø¯Ø§Ø© ØªØ­Ø¶ÙŠØ± Ø£Ø·Ù„Ø³ÙŠØ©:\n"
        "- Ù„Ø§ ØªÙØµØ¯Ø± Ø£Ø­ÙƒØ§Ù… ØµØ­Ø© Ø£Ùˆ Ø¶Ø¹Ù\n"
        "- Ù„Ø§ ØªÙØµÙ„ Ø§Ù„Ø³Ù†Ø¯ ÙˆØ§Ù„Ù…ØªÙ† Ø¢Ù„ÙŠÙ‹Ø§\n"
        "- ØªÙØ³ØªØ®Ø¯Ù… Ù„Ø¨Ù†Ø§Ø¡ Ø¨Ø·Ø§Ù‚Ø© Ø§Ù„Ø­Ø¯ÙŠØ« Ø§Ù„Ø£Ø·Ù„Ø³ÙŠØ©\n\n"
        "ğŸ”¹ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„ØµØ§Ø±Ù… Ù‡Ùˆ Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ Ù„Ø¨Ù†Ø§Ø¡ ÙˆØ­Ø¯Ø© Ø§Ù„Ø­Ø¯ÙŠØ«.\n"
        "ğŸ”¹ ÙŠÙ…ÙƒÙ† ØªØ¹Ø·ÙŠÙ„Ù‡ Ù„Ù„Ø¨Ø­Ø« Ø§Ù„Ø§Ø³ØªÙƒØ´Ø§ÙÙŠ ÙÙ‚Ø·."
    )
