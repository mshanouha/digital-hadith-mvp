import re
import pandas as pd
import streamlit as st

# ======================================================
# CONFIG
# ======================================================
st.set_page_config(page_title="Ø£Ø·Ù„Ø³ Ø§Ù„Ø³Ù†Ø© â€“ Ø§Ù„ØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ø­Ø¯ÙŠØ«ÙŠ", layout="wide")
st.write("VERSION: ATLAS-HADITH v1.0")

# ======================================================
# SESSION STATE
# ======================================================
if "page" not in st.session_state:
    st.session_state.page = "search"

if "active_hadith" not in st.session_state:
    st.session_state.active_hadith = None

if "query_text" not in st.session_state:
    st.session_state.query_text = ""

def go_to_analysis(hadith_key):
    st.session_state.active_hadith = hadith_key
    st.session_state.page = "analysis"
    st.experimental_rerun()

def go_to_search():
    st.session_state.page = "search"
    st.experimental_rerun()

# ======================================================
# ARABIC HELPERS
# ======================================================
AR_DIACRITICS = re.compile(r"[\u0617-\u061A\u064B-\u0652\u0670\u06D6-\u06ED]")

def normalize_ar(text):
    if not text:
        return ""
    text = AR_DIACRITICS.sub("", str(text))
    text = text.replace("Ø£", "Ø§").replace("Ø¥", "Ø§").replace("Ø¢", "Ø§")
    text = text.replace("Ù‰", "ÙŠ").replace("Ø©", "Ù‡")
    text = re.sub(r"[^\w\s\u0600-\u06FF]", " ", text)
    return re.sub(r"\s+", " ", text).strip()

def tokenize_ar(text):
    return normalize_ar(text).split()

def contains_most_words(reference, candidate, threshold=0.8):
    ref_tokens = tokenize_ar(reference)
    cand_tokens = tokenize_ar(candidate)
    if not ref_tokens:
        return False
    shared = sum(1 for tok in ref_tokens if tok in cand_tokens)
    return (shared / len(ref_tokens)) >= threshold

# ======================================================
# DATA LOADING
# ======================================================
@st.cache_data
def load_data():
    df = pd.read_csv("hadith_data.csv")
    required = {"hadith_key", "source", "ref", "isnad", "matn"}
    if not required.issubset(df.columns):
        raise ValueError("Ù…Ù„Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ø§ ÙŠØ­ØªÙˆÙŠ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©")
    return df

df_hadith = load_data()

# ======================================================
# SCORING LOGIC
# ======================================================
def score_color(score):
    if score >= 9:
        return "ğŸŸ¢ Ù‚ÙˆÙŠ Ø¬Ø¯Ù‹Ø§"
    elif score >= 7:
        return "ğŸŸ¢ Ù‚ÙˆÙŠ"
    elif score >= 5:
        return "ğŸŸ¡ Ù…ØªÙˆØ³Ø·"
    elif score >= 3:
        return "ğŸŸ  Ø¶Ø¹ÙŠÙ"
    else:
        return "ğŸ”´ Ø¶Ø¹ÙŠÙ Ø¬Ø¯Ù‹Ø§"

def hadith_global_score(scores):
    if not scores:
        return 0
    max_score = max(scores)
    avg_score = sum(scores) / len(scores)
    strong_paths = len([s for s in scores if s >= 7])
    final = (0.5 * max_score) + (0.3 * avg_score) + (0.2 * strong_paths)
    return round(min(final, 10), 1)

def hadith_description(score):
    if score >= 9:
        return "Ø­Ø¯ÙŠØ« Ø«Ø§Ø¨Øª Ù‚ÙˆÙŠ Ø¬Ø¯Ù‹Ø§"
    elif score >= 7:
        return "Ø­Ø¯ÙŠØ« ØµØ­ÙŠØ­ Ù‚ÙˆÙŠ"
    elif score >= 5:
        return "Ø­Ø¯ÙŠØ« Ø­Ø³Ù† Ø£Ùˆ Ù…ØªÙˆØ³Ø· Ø§Ù„Ù‚ÙˆØ©"
    elif score >= 3:
        return "Ø­Ø¯ÙŠØ« Ø¶Ø¹ÙŠÙ"
    else:
        return "Ø­Ø¯ÙŠØ« Ø¶Ø¹ÙŠÙ Ø¬Ø¯Ù‹Ø§"

# ======================================================
# PAGE: SEARCH
# ======================================================
if st.session_state.page == "search":

    st.title("ğŸ” Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø£Ø·Ù„Ø³ÙŠ Ø¹Ù† Ø§Ù„Ø­Ø¯ÙŠØ«")

    query = st.text_area(
        "Ù†Øµ Ø§Ù„Ø¨Ø­Ø« (ÙƒÙ„ Ø­Ø¯ÙŠØ« ÙÙŠ Ø³Ø·Ø± Ù…Ø³ØªÙ‚Ù„)",
        height=120,
        key="query_text",
        placeholder="Ù…Ø«Ø§Ù„:\nØ¥Ù†Ù…Ø§ Ø§Ù„Ø£Ø¹Ù…Ø§Ù„ Ø¨Ø§Ù„Ù†ÙŠØ§Øª\nÙ…Ù† ÙƒØ°Ø¨ Ø¹Ù„ÙŠ Ù…ØªØ¹Ù…Ø¯Ø§"
    )

    strict_core = st.checkbox("ğŸ”’ Ø¨Ø­Ø« Ø£Ø·Ù„Ø³ÙŠ ØµØ§Ø±Ù… (Ù†ÙˆØ§Ø© Ø§Ù„Ø­Ø¯ÙŠØ« â‰¥ 80Ùª)", value=True)

    if st.button("Ø§Ø¨Ø­Ø«", type="primary"):
        if not query.strip():
            st.warning("Ø§ÙƒØªØ¨ Ù†Øµ Ø§Ù„Ø¨Ø­Ø« Ø£ÙˆÙ„Ù‹Ø§")
        else:
            queries = [q.strip() for q in query.split("\n") if q.strip()]
            results = []

            for q in queries:
                q_norm = normalize_ar(q)
                temp = df_hadith.copy()
                temp["core_match"] = temp["matn"].apply(
                    lambda x: contains_most_words(q_norm, x)
                )
                if strict_core:
                    temp = temp[temp["core_match"]]
                results.append(temp)

            results = pd.concat(results).drop_duplicates()

            if results.empty:
                st.error("Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØªØ§Ø¦Ø¬")
            else:
                st.success(
                    f"ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {results['hadith_key'].nunique()} ÙˆØ­Ø¯Ø© Ø­Ø¯ÙŠØ«ÙŠØ©"
                )

                for hadith_key, grp in results.groupby("hadith_key"):
                    with st.expander(f"ğŸ§­ ÙˆØ­Ø¯Ø© Ø­Ø¯ÙŠØ«: {hadith_key}", expanded=False):
                        st.write("ğŸ“Œ Ø§Ù„Ù…ØªÙ† Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠ:")
                        st.write(grp.iloc[0]["matn"])

                        st.write(f"Ø¹Ø¯Ø¯ Ø§Ù„Ø±ÙˆØ§ÙŠØ§Øª: {len(grp)}")

                        st.button(
                            "ğŸ” Ø§Ù„Ø§Ù†ØªÙ‚Ø§Ù„ Ø¥Ù„Ù‰ Ø§Ù„ØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ø­Ø¯ÙŠØ«ÙŠ",
                            key=f"analyze_{hadith_key}",
                            on_click=go_to_analysis,
                            args=(hadith_key,)
                        )

# ======================================================
# PAGE: ANALYSIS
# ======================================================
if st.session_state.page == "analysis":

    hadith_key = st.session_state.active_hadith
    st.title(f"ğŸ§­ Ø§Ù„ØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ø­Ø¯ÙŠØ«ÙŠ â€“ {hadith_key}")

    hadith_data = df_hadith[df_hadith["hadith_key"] == hadith_key]

    st.subheader("ğŸ“Œ Ø§Ù„Ù†Øµ Ø§Ù„Ù†ÙˆÙˆÙŠ Ù„Ù„Ø­Ø¯ÙŠØ«")
    st.write(hadith_data.iloc[0]["matn"])

    st.markdown("---")
    st.subheader("ğŸ§µ ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø·Ø±Ù‚ Ø§Ù„Ø³Ù†Ø¯ÙŠØ©")

    scores = []

    for idx, row in hadith_data.iterrows():
        with st.container():
            st.markdown(
                f"**Ø§Ù„Ø·Ø±ÙŠÙ‚:** {row['isnad']}  \n"
                f"**Ø§Ù„Ù…ØµØ¯Ø±:** {row['source']} | {row['ref']}"
            )

            score = st.slider(
                "Ø¯Ø±Ø¬Ø© Ø§Ù„Ø·Ø±ÙŠÙ‚ (0â€“10)",
                0, 10, 7,
                key=f"score_{idx}"
            )

            st.write(score_color(score))
            scores.append(score)

            st.markdown("---")

    final_score = hadith_global_score(scores)

    st.subheader("ğŸ“Š Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù„Ù„Ø­Ø¯ÙŠØ«")
    st.metric("Ø§Ù„Ø¯Ø±Ø¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©", f"{final_score} / 10")
    st.write(hadith_description(final_score))

    st.markdown("---")
    st.subheader("ğŸ“ Ø§Ù„Ø­ÙƒÙ… Ø§Ù„Ø­Ø¯ÙŠØ«ÙŠ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ")

    verdict = st.selectbox(
        "Ø§Ù„ØªØµÙ†ÙŠÙ",
        ["ØµØ­ÙŠØ­ Ù‚Ø·Ø¹ÙŠ", "ØµØ­ÙŠØ­", "Ø­Ø³Ù†", "Ù…Ø®ØªÙ„Ù ÙÙŠÙ‡", "Ø¶Ø¹ÙŠÙ"]
    )

    notes = st.text_area("Ø®Ù„Ø§ØµØ© Ø§Ù„ØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ø­Ø¯ÙŠØ«ÙŠ")

    if st.button("ğŸ’¾ Ø­ÙØ¸ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…"):
        st.success("ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… (Ø­Ø§Ù„ÙŠÙ‹Ø§ ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© ÙÙ‚Ø·).")

    st.button("â†©ï¸ Ø§Ù„Ø¹ÙˆØ¯Ø© Ø¥Ù„Ù‰ Ø§Ù„Ø¨Ø­Ø«", on_click=go_to_search)
