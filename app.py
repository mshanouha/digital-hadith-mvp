import re
import pandas as pd
import streamlit as st

# ======================================================
# CONFIG
# ======================================================
st.set_page_config(page_title="Ø£Ø·Ù„Ø³ Ø§Ù„Ø³Ù†Ø©", layout="wide")
st.write("VERSION: ATLAS-HADITH v1.5")

# ======================================================
# SESSION STATE
# ======================================================
if "page" not in st.session_state:
    st.session_state.page = "search"

if "active_hadith" not in st.session_state:
    st.session_state.active_hadith = None

if "similar_results" not in st.session_state:
    st.session_state.similar_results = None


def go(page):
    st.session_state.page = page
    st.rerun()

# ======================================================
# ROUTING (VERY IMPORTANT)
# ======================================================
def route():
    if st.session_state.page == "search":
        page_search()
        st.stop()
    elif st.session_state.page == "unit":
        page_unit()
        st.stop()
    elif st.session_state.page == "analysis":
        page_analysis()
        st.stop()

# ======================================================
# NORMALIZATION
# ======================================================
AR_DIACRITICS = re.compile(r"[\u0617-\u061A\u064B-\u0652\u0670\u06D6-\u06ED]")

def normalize_ar(text):
    if not text:
        return ""
    text = AR_DIACRITICS.sub("", str(text))
    text = text.replace("Ø£", "Ø§").replace("Ø¥", "Ø§").replace("Ø¢", "Ø§")
    text = text.replace("Ù‰", "ÙŠ").replace("Ø©", "Ù‡")
    text = re.sub(r"[^\w\s\u0600-\u06FF]", " ", text)
    return re.sub(r"\s+", " ", text).strip()

def tokenize_ar(text):
    return normalize_ar(text).split()

# ======================================================
# ISNAD
# ======================================================
def normalize_isnad(isnad):
    if not isnad or pd.isna(isnad):
        return None
    isnad = normalize_ar(isnad)
    for w in ["Ø­Ø¯Ø«Ù†Ø§", "Ø§Ø®Ø¨Ø±Ù†Ø§", "Ù‚Ø§Ù„", "Ø³Ù…Ø¹Øª", "Ø¹Ù†"]:
        isnad = isnad.replace(w, "")
    isnad = re.sub(r"\s+", " ", isnad)
    return isnad.strip()

# ======================================================
# STRICT CORE MATCH (ONLY FOR ANALYSIS)
# ======================================================
def contains_core(reference, candidate):
    ref_tokens = tokenize_ar(reference)
    cand_tokens = tokenize_ar(candidate)
    if not ref_tokens:
        return False
    shared = sum(1 for t in ref_tokens if t in cand_tokens)
    if len(ref_tokens) <= 4:
        return shared >= len(ref_tokens) - 1
    return (shared / len(ref_tokens)) >= 0.8

# ======================================================
# DATA
# ======================================================
@st.cache_data
def load_data():
    df = pd.read_csv("hadith_data.csv")
    df["matn_norm"] = df["matn"].apply(normalize_ar)
    df["isnad_norm"] = df["isnad"].apply(normalize_isnad)
    return df

df = load_data()

# ======================================================
# VISUAL BAR (NON INTERACTIVE)
# ======================================================
def render_bar(score):
    colors = [
        "#d73027", "#f46d43", "#fdae61", "#fee08b",
        "#ffffbf", "#d9ef8b", "#a6d96a", "#66bd63",
        "#1a9850", "#006837"
    ]
    active = int(round(score)) - 1
    html = ""
    for i, c in enumerate(colors):
        opacity = "1" if i <= active else "0.25"
        html += f"""
        <div style="
            width:24px;
            height:14px;
            background:{c};
            opacity:{opacity};
            display:inline-block;
            margin:2px;
            border-radius:4px;">
        </div>
        """
    st.markdown(html, unsafe_allow_html=True)

# ======================================================
# PAGE 1 â€” SEARCH
# ======================================================
def page_search():
    st.title("ğŸ” Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø­Ø¯ÙŠØ«")
    st.write("Ø¨Ø­Ø« Ù„ØºÙˆÙŠ Ø¹Ø§Ù…: Ø£Ø¯Ø®Ù„ ÙƒÙ„Ù…Ø© Ø£Ùˆ Ø£ÙƒØ«Ø± (ÙŠØ¬Ø¨ Ø£Ù† ØªØ¬ØªÙ…Ø¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ù…Ø¹Ù‹Ø§).")

    query = st.text_input("Ù†Øµ Ø§Ù„Ø¨Ø­Ø«")

    if st.button("Ø§Ø¨Ø­Ø«", type="primary"):
        query = query.strip()

        if not query:
            st.warning("Ø£Ø¯Ø®Ù„ ÙƒÙ„Ù…Ø© ÙˆØ§Ø­Ø¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„")
            return

        tokens = tokenize_ar(query)

        if not tokens:
            st.warning("Ø§Ù„Ù†Øµ ØºÙŠØ± ØµØ§Ù„Ø­ Ù„Ù„Ø¨Ø­Ø«")
            return

        # =========================
        # AND SEARCH (all words)
        # =========================
        results = df.copy()
        for tok in tokens:
            results = results[
                results["matn_norm"].str.contains(tok, regex=False)
            ]

        if results.empty:
            st.info("ğŸ” Ù„Ù… ØªÙˆØ¬Ø¯ Ø¹Ø¨Ø§Ø±Ø© ÙƒØ§Ù…Ù„Ø©ØŒ ØªÙ… Ø§Ù„Ø¨Ø­Ø« Ø¨Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØ±Ø¯Ø©")

            # fallback OR search (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
            pattern = "|".join(tokens)
            results = df[df["matn_norm"].str.contains(pattern, regex=True)]

            if results.empty:
                st.error("Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØªØ§Ø¦Ø¬")
                return

        else:
            st.success("ğŸ” ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬ Ù…Ø·Ø§Ø¨Ù‚Ø© Ù„Ù„Ø¹Ø¨Ø§Ø±Ø© ÙƒØ§Ù…Ù„Ø©")

        # =========================
        # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        # =========================
        for key, grp in results.groupby("hadith_key"):
            with st.expander(f"ğŸ§­ Ø­Ø¯ÙŠØ«: {key}"):
                st.write(grp.iloc[0]["matn"])

                if st.button("ğŸ§­ Ø§Ø®ØªÙŠØ§Ø± Ù‡Ø°Ø§ Ø§Ù„Ø­Ø¯ÙŠØ«", key=f"sel_{key}"):
                    st.session_state.active_hadith = key
                    st.session_state.page = "unit"
                    st.rerun()



# ======================================================
# PAGE 2 â€” HADITH UNIT
# ======================================================
def page_unit():
    key = st.session_state.active_hadith
    data = df[df["hadith_key"] == key]

    st.title(f"ğŸ§­ ÙˆØ­Ø¯Ø© Ø§Ù„Ø­Ø¯ÙŠØ« â€“ {key}")
    st.markdown("### ğŸ“Œ Ø§Ù„Ù…ØªÙ† Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠ")
    st.write(data.iloc[0]["matn"])

    st.markdown("---")
    col1, col2 = st.columns(2)

    with col1:
        if st.button("ğŸ” Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…ØªØ´Ø§Ø¨Ù‡"):
            ref = data.iloc[0]["matn"]
            sim = df[df["matn"].apply(lambda x: contains_core(ref, x))]
            st.session_state.similar_results = sim
            go("analysis")

    with col2:
        if st.button("ğŸ“Š Ø§Ù„Ø§Ù†ØªÙ‚Ø§Ù„ Ø¥Ù„Ù‰ Ø§Ù„ØªØ­Ù‚ÙŠÙ‚"):
            st.session_state.similar_results = data
            go("analysis")

    st.markdown("---")
    if st.button("â†©ï¸ Ø§Ù„Ø¹ÙˆØ¯Ø© Ù„Ù„Ø¨Ø­Ø«"):
        go("search")

# ======================================================
# PAGE 3 â€” ANALYSIS
# ======================================================
def page_analysis():
    data = st.session_state.similar_results

    st.title("ğŸ”¬ Ø§Ù„ØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ø­Ø¯ÙŠØ«ÙŠ ÙˆØ§Ù„Ù…ØªØ´Ø§Ø¨Ù‡")

    scores = []

    for isnad, grp in data.groupby("isnad_norm"):
        st.markdown(f"**Ø§Ù„Ø³Ù†Ø¯:** {isnad}")
        score = st.selectbox(
            "Ø¯Ø±Ø¬Ø© Ø§Ù„Ø·Ø±ÙŠÙ‚ (0â€“10)",
            list(range(11)),
            index=7,
            key=f"s_{isnad}"
        )
        render_bar(score)
        scores.append(score)
        st.markdown("---")

    if scores:
        final = round(sum(scores) / len(scores), 1)
        st.subheader("ğŸ“Š Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù„Ù„Ø­Ø¯ÙŠØ«")
        st.write(f"Ø§Ù„Ø¯Ø±Ø¬Ø©: {final} / 10")
        render_bar(final)

    if st.button("â†©ï¸ Ø§Ù„Ø¹ÙˆØ¯Ø© Ù„ÙˆØ­Ø¯Ø© Ø§Ù„Ø­Ø¯ÙŠØ«"):
        go("unit")

# ======================================================
# RUN APP
# ======================================================
route()
